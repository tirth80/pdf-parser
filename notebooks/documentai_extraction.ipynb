{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Document AI PDF Parser\n",
        "\n",
        "This notebook demonstrates how to use Google Cloud Document AI to parse PDF documents, extract text, tables, form fields, and entities with cost estimation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Optional, List, Dict\n",
        "import pandas as pd\n",
        "from google.cloud import documentai_v1 as documentai\n",
        "from google.oauth2 import service_account\n",
        "import time\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document AI Parser Class with Cost Estimation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentAIParser:\n",
        "    def __init__(\n",
        "        self,\n",
        "        project_id: str,\n",
        "        location: str,\n",
        "        processor_id: str,\n",
        "        credentials_path: str\n",
        "    ):\n",
        "        \"\"\"Initialize Document AI client with credentials.\"\"\"\n",
        "        # Set up credentials\n",
        "        credentials = service_account.Credentials.from_service_account_file(\n",
        "            credentials_path\n",
        "        )\n",
        "        \n",
        "        # Initialize client\n",
        "        self.client = documentai.DocumentProcessorServiceClient(\n",
        "            credentials=credentials\n",
        "        )\n",
        "        \n",
        "        # Set processor path\n",
        "        self.processor_name = self.client.processor_path(\n",
        "            project_id, location, processor_id\n",
        "        )\n",
        "        \n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        \n",
        "        # Cost tracking\n",
        "        self.cost_per_page = 0.0015  # $0.0015 per page (as of 2024)\n",
        "        self.cost_per_1000_chars = 0.0001  # $0.0001 per 1000 characters\n",
        "        self.total_cost = 0.0\n",
        "        self.processing_stats = {\n",
        "            'pages_processed': 0,\n",
        "            'characters_processed': 0,\n",
        "            'tables_extracted': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'form_fields_extracted': 0\n",
        "        }\n",
        "        \n",
        "    def parse_pdf(self, file_path: str) -> documentai.Document:\n",
        "        \"\"\"Parse a PDF file using Document AI with cost tracking.\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Read file and get size info\n",
        "        file_size = os.path.getsize(file_path)\n",
        "        print(f\"Processing file: {file_path} ({file_size:,} bytes)\")\n",
        "        \n",
        "        with open(file_path, 'rb') as file:\n",
        "            file_content = file.read()\n",
        "        \n",
        "        # Configure the process request\n",
        "        document = documentai.RawDocument(\n",
        "            content=file_content,\n",
        "            mime_type='application/pdf'\n",
        "        )\n",
        "        \n",
        "        request = documentai.ProcessRequest(\n",
        "            name=self.processor_name,\n",
        "            raw_document=document\n",
        "        )\n",
        "        \n",
        "        # Process the document\n",
        "        print(\"Sending request to Document AI...\")\n",
        "        result = self.client.process_document(request=request)\n",
        "        \n",
        "        # Calculate costs\n",
        "        processing_time = time.time() - start_time\n",
        "        pages = len(result.document.pages)\n",
        "        characters = len(result.document.text)\n",
        "        \n",
        "        # Update stats\n",
        "        self.processing_stats['pages_processed'] += pages\n",
        "        self.processing_stats['characters_processed'] += characters\n",
        "        \n",
        "        # Calculate cost\n",
        "        page_cost = pages * self.cost_per_page\n",
        "        char_cost = (characters / 1000) * self.cost_per_1000_chars\n",
        "        total_doc_cost = page_cost + char_cost\n",
        "        self.total_cost += total_doc_cost\n",
        "        \n",
        "        print(f\"\\nProcessing completed in {processing_time:.2f} seconds\")\n",
        "        print(f\"Pages processed: {pages}\")\n",
        "        print(f\"Characters extracted: {characters:,}\")\n",
        "        print(f\"Estimated cost: ${total_doc_cost:.4f}\")\n",
        "        \n",
        "        return result.document\n",
        "    \n",
        "    def extract_text(self, document: documentai.Document) -> str:\n",
        "        \"\"\"Extract all text from parsed document.\"\"\"\n",
        "        return document.text\n",
        "    \n",
        "    def extract_tables(self, document: documentai.Document) -> List[pd.DataFrame]:\n",
        "        \"\"\"Extract tables as pandas DataFrames.\"\"\"\n",
        "        tables = []\n",
        "        \n",
        "        for page in document.pages:\n",
        "            for table in page.tables:\n",
        "                header_rows = []\n",
        "                body_rows = []\n",
        "                \n",
        "                for row_idx, row in enumerate(table.header_rows):\n",
        "                    header_row = []\n",
        "                    for cell in row.cells:\n",
        "                        cell_text = self._get_text_from_layout(\n",
        "                            cell.layout, document.text\n",
        "                        )\n",
        "                        header_row.append(cell_text)\n",
        "                    header_rows.append(header_row)\n",
        "                \n",
        "                for row_idx, row in enumerate(table.body_rows):\n",
        "                    body_row = []\n",
        "                    for cell in row.cells:\n",
        "                        cell_text = self._get_text_from_layout(\n",
        "                            cell.layout, document.text\n",
        "                        )\n",
        "                        body_row.append(cell_text)\n",
        "                    body_rows.append(body_row)\n",
        "                \n",
        "                # Create DataFrame\n",
        "                if header_rows and body_rows:\n",
        "                    df = pd.DataFrame(body_rows, columns=header_rows[0])\n",
        "                    tables.append(df)\n",
        "                elif body_rows:\n",
        "                    df = pd.DataFrame(body_rows)\n",
        "                    tables.append(df)\n",
        "        \n",
        "        self.processing_stats['tables_extracted'] += len(tables)\n",
        "        return tables\n",
        "    \n",
        "    def extract_form_fields(self, document: documentai.Document) -> Dict[str, str]:\n",
        "        \"\"\"Extract form fields as key-value pairs.\"\"\"\n",
        "        fields = {}\n",
        "        \n",
        "        for page in document.pages:\n",
        "            for field in page.form_fields:\n",
        "                field_name = self._get_text_from_layout(\n",
        "                    field.field_name, document.text\n",
        "                )\n",
        "                field_value = self._get_text_from_layout(\n",
        "                    field.field_value, document.text\n",
        "                )\n",
        "                fields[field_name] = field_value\n",
        "        \n",
        "        self.processing_stats['form_fields_extracted'] += len(fields)\n",
        "        return fields\n",
        "    \n",
        "    def extract_entities(self, document: documentai.Document) -> List[Dict]:\n",
        "        \"\"\"Extract entities identified by Document AI.\"\"\"\n",
        "        entities = []\n",
        "        \n",
        "        for entity in document.entities:\n",
        "            entities.append({\n",
        "                'type': entity.type_,\n",
        "                'text': entity.mention_text,\n",
        "                'confidence': entity.confidence\n",
        "            })\n",
        "        \n",
        "        self.processing_stats['entities_extracted'] += len(entities)\n",
        "        return entities\n",
        "    \n",
        "    def _get_text_from_layout(\n",
        "        self, \n",
        "        layout: documentai.Document.Page.Layout,\n",
        "        text: str\n",
        "    ) -> str:\n",
        "        \"\"\"Extract text from layout element.\"\"\"\n",
        "        response = \"\"\n",
        "        \n",
        "        for segment in layout.text_anchor.text_segments:\n",
        "            start_index = int(segment.start_index) if segment.start_index else 0\n",
        "            end_index = int(segment.end_index)\n",
        "            response += text[start_index:end_index]\n",
        "        \n",
        "        return response.strip()\n",
        "    \n",
        "    def get_cost_summary(self) -> Dict:\n",
        "        \"\"\"Get detailed cost and processing summary.\"\"\"\n",
        "        return {\n",
        "            'total_cost_usd': round(self.total_cost, 4),\n",
        "            'processing_stats': self.processing_stats.copy(),\n",
        "            'cost_breakdown': {\n",
        "                'pages_cost': round(self.processing_stats['pages_processed'] * self.cost_per_page, 4),\n",
        "                'characters_cost': round((self.processing_stats['characters_processed'] / 1000) * self.cost_per_1000_chars, 4)\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def reset_cost_tracking(self):\n",
        "        \"\"\"Reset cost tracking for new session.\"\"\"\n",
        "        self.total_cost = 0.0\n",
        "        self.processing_stats = {\n",
        "            'pages_processed': 0,\n",
        "            'characters_processed': 0,\n",
        "            'tables_extracted': 0,\n",
        "            'entities_extracted': 0,\n",
        "            'form_fields_extracted': 0\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "**IMPORTANT**: Update these configuration values with your actual Google Cloud credentials and settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "Project ID: marine-actor-473300-h8\n",
            "Location: us\n",
            "Processor ID: 2258be222035edb3\n",
            "Credentials: /Users/divyanshmac/Documents/Google Cloud/credentials.json\n",
            "PDF Path: ../data/GCP_EXTRACTION.pdf\n"
          ]
        }
      ],
      "source": [
        "# Configuration - UPDATE THESE VALUES\n",
        "PROJECT_ID = \"marine-actor-473300-h8\"  # Your Google Cloud Project ID\n",
        "LOCATION = \"us\"  # or \"eu\" - your processor location\n",
        "PROCESSOR_ID = \"2258be222035edb3\"  # Your Document AI processor ID\n",
        "CREDENTIALS_PATH = \"/Users/divyanshmac/Documents/Google Cloud/credentials.json\"  # Path to your service account key file\n",
        "\n",
        "# PDF file to process - UPDATE THIS PATH\n",
        "PDF_PATH = \"../data/GCP_EXTRACTION.pdf\"  # Path to the PDF you want to parse\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Location: {LOCATION}\")\n",
        "print(f\"Processor ID: {PROCESSOR_ID}\")\n",
        "print(f\"Credentials: {CREDENTIALS_PATH}\")\n",
        "print(f\"PDF Path: {PDF_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Document AI Parser initialized successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1758851588.521690 20886014 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Initialize parser\n",
        "try:\n",
        "    parser = DocumentAIParser(\n",
        "        project_id=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        processor_id=PROCESSOR_ID,\n",
        "        credentials_path=CREDENTIALS_PATH\n",
        "    )\n",
        "    print(\"✅ Document AI Parser initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error initializing parser: {e}\")\n",
        "    print(\"Please check your configuration values and credentials.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse PDF Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting PDF parsing...\n",
            "Processing file: ../data/GCP_EXTRACTION.pdf (308,151 bytes)\n",
            "Sending request to Document AI...\n",
            "\n",
            "Processing completed in 5.90 seconds\n",
            "Pages processed: 5\n",
            "Characters extracted: 12,824\n",
            "Estimated cost: $0.0088\n",
            "✅ PDF parsed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Parse PDF\n",
        "try:\n",
        "    print(\"Starting PDF parsing...\")\n",
        "    document = parser.parse_pdf(PDF_PATH)\n",
        "    print(\"✅ PDF parsed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error parsing PDF: {e}\")\n",
        "    print(\"Please check your PDF path and processor configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Text Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 12,824 characters of text\n",
            "First 500 characters:\n",
            "PARTI\n",
            "Item 1B, 1C\n",
            "ITEM 1B. UNRESOLVED STAFF COMMENTS\n",
            "We have received no written comments regarding our periodic or current reports from the staff of the Securities and\n",
            "Exchange Commission that were issued 180 days or more preceding the end of our fiscal year 2024 that remain\n",
            "unresolved.\n",
            "ITEM 1C. CYBERSECURITY\n",
            "RISK MANAGEMENT AND STRATEGY\n",
            "Microsoft plays a central role in the world's digital ecosystem. We have made it the top corporate priority to protect the\n",
            "computing environment used by our cu...\n",
            "✅ Text saved to ../data/parsed/documentai_output/extracted_text.txt\n"
          ]
        }
      ],
      "source": [
        "# Extract text\n",
        "if 'document' in locals():\n",
        "    text = parser.extract_text(document)\n",
        "    print(f\"Extracted {len(text):,} characters of text\")\n",
        "    print(f\"First 500 characters:\\n{text[:500]}...\")\n",
        "    \n",
        "    # Save text to file\n",
        "    output_dir = Path(\"../data/parsed/documentai_output\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    with open(output_dir / \"extracted_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"✅ Text saved to {output_dir / 'extracted_text.txt'}\")\n",
        "else:\n",
        "    print(\"❌ No document available. Please run the parsing cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 tables\n",
            "\n",
            "Table 1:\n",
            "Shape: (2, 1)\n",
            "                      Location\\nOwned\\nLeased\\nTotal\n",
            "0  U.S.\\n30\\n20\\n50\\nFi\\n30\\nInternational\\n20\\n1...\n",
            "1                                  Total\\n40\\n45\\n85\n",
            "✅ Saved to ../data/parsed/documentai_output/table_1.csv\n",
            "\n",
            "Table 2:\n",
            "Shape: (2, 5)\n",
            "  Declaration Date      Record Date        Payment Date Dividend\\nPer Share  \\\n",
            "0                                                                             \n",
            "1    June 12, 2024  August 15, 2024  September 12, 2024                0.75   \n",
            "\n",
            "          Amount  \n",
            "0  (In millions)  \n",
            "1       $\\n5,575  \n",
            "✅ Saved to ../data/parsed/documentai_output/table_2.csv\n",
            "\n",
            "Table 3:\n",
            "Shape: (5, 5)\n",
            "                           Period Total Number\\nof Shares\\nPurchased  \\\n",
            "0                                                                      \n",
            "1  April 1, 2024 - April 30, 2024                          2,444,905   \n",
            "2      May 1, 2024 - May 31, 2024                          2,233,450   \n",
            "3    June 1, 2024 - June 30, 2024                          1,963,873   \n",
            "4                                                          6,642,228   \n",
            "\n",
            "  Average Price\\nPaid Per Share  \\\n",
            "0                                 \n",
            "1                     $\\n413.75   \n",
            "2                        416.85   \n",
            "3                        436.58   \n",
            "4                                 \n",
            "\n",
            "  Total Number of\\nShares Purchased\\nas part of Publicly\\nAnnounced Plans\\nor Programs  \\\n",
            "0                                                                                        \n",
            "1                                          2,444,905                                     \n",
            "2                                          2,233,450                                     \n",
            "3                                          1,963,873                                     \n",
            "4                                          6,642,228                                     \n",
            "\n",
            "  Approximate Dollar\\nValue of Shares That\\nMay Yet Be\\nPurchased Under the\\nPlans or Programs  \n",
            "0                                      (In millions)                                            \n",
            "1                                          $\\n12,138                                            \n",
            "2                                             11,207                                            \n",
            "3                                             10,349                                            \n",
            "4                                                                                               \n",
            "✅ Saved to ../data/parsed/documentai_output/table_3.csv\n"
          ]
        }
      ],
      "source": [
        "# Extract tables\n",
        "if 'document' in locals():\n",
        "    tables = parser.extract_tables(document)\n",
        "    print(f\"Found {len(tables)} tables\")\n",
        "    \n",
        "    if tables:\n",
        "        # Save tables to CSV files\n",
        "        output_dir = Path(\"../data/parsed/documentai_output\")\n",
        "        \n",
        "        for i, table in enumerate(tables):\n",
        "            print(f\"\\nTable {i+1}:\")\n",
        "            print(f\"Shape: {table.shape}\")\n",
        "            print(table.head())\n",
        "            \n",
        "            # Save to CSV\n",
        "            csv_path = output_dir / f\"table_{i+1}.csv\"\n",
        "            table.to_csv(csv_path, index=False)\n",
        "            print(f\"✅ Saved to {csv_path}\")\n",
        "    else:\n",
        "        print(\"No tables found in the document.\")\n",
        "else:\n",
        "    print(\"❌ No document available. Please run the parsing cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Form Fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5 form fields:\n",
            "  Secure by Design:: Security comes first when designing any product or service.\n",
            "  Declaration Date: June 12, 2024\n",
            "  Dividend\n",
            "Per Share: Amount\n",
            "  Record Date: Declaration Date\n",
            "  (In millions): 5,575\n",
            "\n",
            "✅ Form fields saved to ../data/parsed/documentai_output/form_fields.json\n"
          ]
        }
      ],
      "source": [
        "# Extract form fields\n",
        "if 'document' in locals():\n",
        "    fields = parser.extract_form_fields(document)\n",
        "    \n",
        "    if fields:\n",
        "        print(f\"Found {len(fields)} form fields:\")\n",
        "        for key, value in fields.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "        \n",
        "        # Save form fields to JSON\n",
        "        output_dir = Path(\"../data/parsed/documentai_output\")\n",
        "        with open(output_dir / \"form_fields.json\", \"w\") as f:\n",
        "            json.dump(fields, f, indent=2)\n",
        "        print(f\"\\n✅ Form fields saved to {output_dir / 'form_fields.json'}\")\n",
        "    else:\n",
        "        print(\"No form fields found in the document.\")\n",
        "else:\n",
        "    print(\"❌ No document available. Please run the parsing cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 entities:\n",
            "\n",
            "generic_entities (4 found):\n",
            "  -  (confidence: 0.00)\n",
            "  -  (confidence: 0.00)\n",
            "  -  (confidence: 0.00)\n",
            "  -  (confidence: 0.00)\n",
            "\n",
            "✅ Entities saved to ../data/parsed/documentai_output/entities.json\n"
          ]
        }
      ],
      "source": [
        "# Extract entities\n",
        "if 'document' in locals():\n",
        "    entities = parser.extract_entities(document)\n",
        "    \n",
        "    if entities:\n",
        "        print(f\"Found {len(entities)} entities:\")\n",
        "        \n",
        "        # Group entities by type\n",
        "        entity_types = {}\n",
        "        for entity in entities:\n",
        "            entity_type = entity['type']\n",
        "            if entity_type not in entity_types:\n",
        "                entity_types[entity_type] = []\n",
        "            entity_types[entity_type].append(entity)\n",
        "        \n",
        "        for entity_type, type_entities in entity_types.items():\n",
        "            print(f\"\\n{entity_type} ({len(type_entities)} found):\")\n",
        "            for entity in type_entities[:5]:  # Show first 5 of each type\n",
        "                print(f\"  - {entity['text']} (confidence: {entity['confidence']:.2f})\")\n",
        "            if len(type_entities) > 5:\n",
        "                print(f\"  ... and {len(type_entities) - 5} more\")\n",
        "        \n",
        "        # Save entities to JSON\n",
        "        output_dir = Path(\"../data/parsed/documentai_output\")\n",
        "        with open(output_dir / \"entities.json\", \"w\") as f:\n",
        "            json.dump(entities, f, indent=2)\n",
        "        print(f\"\\n✅ Entities saved to {output_dir / 'entities.json'}\")\n",
        "    else:\n",
        "        print(\"No entities found in the document.\")\n",
        "else:\n",
        "    print(\"❌ No document available. Please run the parsing cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Processing Summary:\n",
            "Total Cost: $0.0088\n",
            "Pages Processed: 5\n",
            "Characters Processed: 12,824\n",
            "Tables Extracted: 3\n",
            "Form Fields Extracted: 5\n",
            "Entities Extracted: 4\n",
            "\n",
            "💰 Cost Breakdown:\n",
            "Pages Cost: $0.0075\n",
            "Characters Cost: $0.0013\n",
            "\n",
            "✅ Cost summary saved to ../data/parsed/documentai_output/cost_summary.json\n"
          ]
        }
      ],
      "source": [
        "# Get cost summary\n",
        "cost_summary = parser.get_cost_summary()\n",
        "\n",
        "print(\"📊 Processing Summary:\")\n",
        "print(f\"Total Cost: ${cost_summary['total_cost_usd']}\")\n",
        "print(f\"Pages Processed: {cost_summary['processing_stats']['pages_processed']}\")\n",
        "print(f\"Characters Processed: {cost_summary['processing_stats']['characters_processed']:,}\")\n",
        "print(f\"Tables Extracted: {cost_summary['processing_stats']['tables_extracted']}\")\n",
        "print(f\"Form Fields Extracted: {cost_summary['processing_stats']['form_fields_extracted']}\")\n",
        "print(f\"Entities Extracted: {cost_summary['processing_stats']['entities_extracted']}\")\n",
        "\n",
        "print(\"\\n💰 Cost Breakdown:\")\n",
        "print(f\"Pages Cost: ${cost_summary['cost_breakdown']['pages_cost']}\")\n",
        "print(f\"Characters Cost: ${cost_summary['cost_breakdown']['characters_cost']}\")\n",
        "\n",
        "# Save cost summary\n",
        "output_dir = Path(\"../data/parsed/documentai_output\")\n",
        "with open(output_dir / \"cost_summary.json\", \"w\") as f:\n",
        "    json.dump(cost_summary, f, indent=2)\n",
        "print(f\"\\n✅ Cost summary saved to {output_dir / 'cost_summary.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reset for New Document\n",
        "\n",
        "Run this cell to reset cost tracking before processing a new document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset cost tracking\n",
        "parser.reset_cost_tracking()\n",
        "print(\"✅ Cost tracking reset. Ready for new document processing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### Setup Requirements:\n",
        "1. **Google Cloud Project**: Create a project in Google Cloud Console\n",
        "2. **Enable Document AI API**: Enable the Document AI API in your project\n",
        "3. **Create Processor**: Create a Document AI processor (Form Parser, OCR, etc.)\n",
        "4. **Service Account**: Create a service account and download the JSON key file\n",
        "5. **Update Configuration**: Update the configuration cell with your actual values\n",
        "\n",
        "### Cost Information:\n",
        "- **Per Page**: $0.0015 USD\n",
        "- **Per 1000 Characters**: $0.0001 USD\n",
        "- Costs are estimated based on current Google Cloud pricing (2024)\n",
        "\n",
        "### Output Files:\n",
        "All extracted data is saved to `../data/parsed/documentai_output/`:\n",
        "- `extracted_text.txt`: Full text content\n",
        "- `table_*.csv`: Extracted tables as CSV files\n",
        "- `form_fields.json`: Form fields as key-value pairs\n",
        "- `entities.json`: Extracted entities with confidence scores\n",
        "- `cost_summary.json`: Processing statistics and cost breakdown\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
