{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d47325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171991bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Can be local file path or URL\n",
    "pdf_dir = \"../data/raw/MSFT/10-K/PDFs/\"\n",
    "\n",
    "pdf_files = os.listdir(pdf_dir)\n",
    "pdf_src = os.path.join(pdf_dir, pdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae8ecd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw/MSFT/10-K/PDFs/MSFT_10-K_20230727_000095017023035122.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5902ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_table_continuations(tables_metadata):\n",
    "    \"\"\"Detect tables that might be continuations across pages.\"\"\"\n",
    "    # Group by similar structure (columns, headers)\n",
    "    continuation_groups = {}\n",
    "    group_counter = 1\n",
    "    \n",
    "    for table_meta in tables_metadata:\n",
    "        # Simple heuristic: same number of columns and similar headers\n",
    "        key = f\"{table_meta['num_columns']}_{table_meta['num_rows']}\"\n",
    "        \n",
    "        if key not in continuation_groups:\n",
    "            continuation_groups[key] = f\"group_{group_counter}\"\n",
    "            group_counter += 1\n",
    "        \n",
    "        table_meta['continuation_group'] = continuation_groups[key]\n",
    "    \n",
    "    return tables_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54281493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_file_hash(file_path):\n",
    "    \"\"\"Compute SHA-256 hash of file content.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(chunk)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def compute_table_id(file_id, page_number, table_index, mode, params_hash):\n",
    "    \"\"\"Compute deterministic table ID.\"\"\"\n",
    "    content = f\"{file_id}_{page_number}_{table_index}_{mode}_{params_hash}\"\n",
    "    return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "def get_tabula_version():\n",
    "    \"\"\"Get tabula-py version.\"\"\"\n",
    "    try:\n",
    "        import tabula\n",
    "        return getattr(tabula, '__version__', 'unknown')\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def create_file_metadata(pdf_path, output_dir, tables_found, tables_valid, status=\"success\"):\n",
    "    \"\"\"Create file-level metadata.\"\"\"\n",
    "    file_id = compute_file_hash(pdf_path)\n",
    "    file_size = os.path.getsize(pdf_path)\n",
    "    \n",
    "    return {\n",
    "        \"file_id\": file_id,\n",
    "        \"source_path\": pdf_path,\n",
    "        \"filename\": os.path.basename(pdf_path),\n",
    "        \"file_size_bytes\": file_size,\n",
    "        \"extraction_timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"tabula_version\": get_tabula_version(),\n",
    "        \"mode\": \"stream\",\n",
    "        \"tables_found\": tables_found,\n",
    "        \"tables_valid\": tables_valid,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"processing_status\": status\n",
    "    }\n",
    "\n",
    "def create_table_metadata(file_id, table_index, csv_path, page_number, table, \n",
    "                         is_valid, validation_reason, continuation_group=None, \n",
    "                         is_continuation=False, parent_table_id=None):\n",
    "    \"\"\"Create table-level metadata.\"\"\"\n",
    "    # Compute table ID\n",
    "    params_hash = hashlib.sha256(\"stream_mode\".encode()).hexdigest()[:8]\n",
    "    table_id = compute_table_id(file_id, page_number, table_index, \"stream\", params_hash)\n",
    "    \n",
    "    # Compute CSV hash\n",
    "    csv_hash = compute_file_hash(csv_path) if os.path.exists(csv_path) else None\n",
    "    \n",
    "    # Basic structure info\n",
    "    num_rows = len(table) if table is not None else 0\n",
    "    num_columns = len(table.columns) if table is not None and not table.empty else 0\n",
    "    \n",
    "    # Quality metrics\n",
    "    numeric_cell_ratio = 0.0\n",
    "    empty_cell_ratio = 0.0\n",
    "    \n",
    "    if table is not None and not table.empty:\n",
    "        total_cells = num_rows * num_columns\n",
    "        if total_cells > 0:\n",
    "            # Count numeric cells\n",
    "            numeric_cells = table.applymap(\n",
    "                lambda x: str(x).replace(\".\", \"\", 1).isdigit()\n",
    "            ).sum().sum()\n",
    "            numeric_cell_ratio = numeric_cells / total_cells\n",
    "            \n",
    "            # Count empty cells\n",
    "            empty_cells = table.isnull().sum().sum()\n",
    "            empty_cell_ratio = empty_cells / total_cells\n",
    "    \n",
    "    return {\n",
    "        \"table_id\": table_id,\n",
    "        \"file_id\": file_id,\n",
    "        \"table_index\": table_index,\n",
    "        \"csv_path\": csv_path,\n",
    "        \"csv_sha256\": csv_hash,\n",
    "        \"page_number\": page_number,\n",
    "        \"num_rows\": num_rows,\n",
    "        \"num_columns\": num_columns,\n",
    "        \"is_valid_table\": is_valid,\n",
    "        \"validation_reason\": validation_reason,\n",
    "        \"numeric_cell_ratio\": round(numeric_cell_ratio, 3),\n",
    "        \"empty_cell_ratio\": round(empty_cell_ratio, 3),\n",
    "        \"continuation_group\": continuation_group,\n",
    "        \"is_continuation\": is_continuation,\n",
    "        \"parent_table_id\": parent_table_id,\n",
    "        \"extraction_timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edd42093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_table(df):\n",
    "    \"\"\"Heuristic to filter real tables.\"\"\"\n",
    "    # Skip empty frames\n",
    "    if df is None or df.empty:\n",
    "        return False, \"empty_table\"\n",
    "    \n",
    "    # Skip if only one column (likely a paragraph)\n",
    "    if df.shape[1] <= 1:\n",
    "        return False, \"single_column\"\n",
    "    \n",
    "    # Keep if table has any numeric values\n",
    "    has_numbers = df.map(\n",
    "        lambda x: str(x).replace(\".\", \"\", 1).isdigit()\n",
    "    ).any().any()\n",
    "    \n",
    "    if has_numbers:\n",
    "        return True, \"has_numbers\"\n",
    "    elif df.shape[1] > 1:\n",
    "        return True, \"multi_column\"\n",
    "    else:\n",
    "        return False, \"no_numbers_single_column\"\n",
    "\n",
    "def export_tables_to_csv_with_metadata(pdf_path: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Extracts all tables from a PDF (stream mode) and saves only real tables as separate CSV files.\n",
    "    Also generates metadata for files and tables.\n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF.\n",
    "        output_dir (str): Directory where CSV files will be saved.\n",
    "    Returns:\n",
    "        tuple: (csv_files, file_metadata, tables_metadata)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create metadata directory\n",
    "    metadata_dir = os.path.join(output_dir, \"metadata\")\n",
    "    os.makedirs(metadata_dir, exist_ok=True)\n",
    "\n",
    "    # Use stream mode (better for borderless tables)\n",
    "    tables = tabula.read_pdf(\n",
    "        pdf_path,\n",
    "        pages=\"all\",\n",
    "        multiple_tables=True,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Compute file ID once\n",
    "    file_id = compute_file_hash(pdf_path)\n",
    "    \n",
    "    output_files = []\n",
    "    tables_metadata = []\n",
    "    valid_table_count = 0\n",
    "    \n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # Check if table is valid and get reason\n",
    "        is_valid, validation_reason = is_valid_table(table)\n",
    "        \n",
    "        if is_valid:\n",
    "            output_file = os.path.join(output_dir, f\"table_{i}.csv\")\n",
    "            table.to_csv(output_file, index=False)\n",
    "            output_files.append(output_file)\n",
    "            valid_table_count += 1\n",
    "        \n",
    "        # Create metadata for all tables (valid and invalid)\n",
    "        # Note: We don't have page numbers from tabula directly, so we'll use table index as proxy\n",
    "        # In a more sophisticated version, you'd extract page info from tabula's area detection\n",
    "        table_meta = create_table_metadata(\n",
    "            file_id=file_id,\n",
    "            table_index=i,\n",
    "            csv_path=os.path.join(output_dir, f\"table_{i}.csv\") if is_valid else None,\n",
    "            page_number=i,  # Simplified - tabula doesn't always provide exact page info\n",
    "            table=table,\n",
    "            is_valid=is_valid,\n",
    "            validation_reason=validation_reason\n",
    "        )\n",
    "        tables_metadata.append(table_meta)\n",
    "    \n",
    "    # Create file-level metadata\n",
    "    file_metadata = create_file_metadata(\n",
    "        pdf_path=pdf_path,\n",
    "        output_dir=output_dir,\n",
    "        tables_found=len(tables),\n",
    "        tables_valid=valid_table_count\n",
    "    )\n",
    "    \n",
    "    # Detect continuations\n",
    "    tables_metadata = detect_table_continuations(tables_metadata)\n",
    "    \n",
    "    # Save metadata files\n",
    "    with open(os.path.join(metadata_dir, \"files.jsonl\"), \"w\") as f:\n",
    "        f.write(json.dumps(file_metadata) + \"\\n\")\n",
    "    \n",
    "    with open(os.path.join(metadata_dir, \"tables.jsonl\"), \"w\") as f:\n",
    "        for table_meta in tables_metadata:\n",
    "            f.write(json.dumps(table_meta) + \"\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_dir}\")\n",
    "    print(f\"File metadata: {file_metadata['file_id']}\")\n",
    "    print(f\"Tables metadata: {len(tables_metadata)} records\")\n",
    "    \n",
    "    return output_files, file_metadata, tables_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9f82ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/4kkvfwq1597_qj23_z5nkrdm0000gn/T/ipykernel_62130/2869176935.py:64: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  numeric_cells = table.applymap(\n",
      "/var/folders/9v/4kkvfwq1597_qj23_z5nkrdm0000gn/T/ipykernel_62130/2869176935.py:89: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"extraction_timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
      "/var/folders/9v/4kkvfwq1597_qj23_z5nkrdm0000gn/T/ipykernel_62130/2869176935.py:64: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  numeric_cells = table.applymap(\n",
      "/var/folders/9v/4kkvfwq1597_qj23_z5nkrdm0000gn/T/ipykernel_62130/2869176935.py:89: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"extraction_timestamp\": datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m pdf_file = pdf_src\n\u001b[32m      4\u001b[39m save_dir = \u001b[33m\"\u001b[39m\u001b[33m../data/parsed/tabula_output\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m csv_files, file_metadata, tables_metadata = \u001b[43mexport_tables_to_csv_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExported CSV files:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m csv_files:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mexport_tables_to_csv_with_metadata\u001b[39m\u001b[34m(pdf_path, output_dir)\u001b[39m\n\u001b[32m     62\u001b[39m         valid_table_count += \u001b[32m1\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Create metadata for all tables (valid and invalid)\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Note: We don't have page numbers from tabula directly, so we'll use table index as proxy\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# In a more sophisticated version, you'd extract page info from tabula's area detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     table_meta = \u001b[43mcreate_table_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtable_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Simplified - tabula doesn't always provide exact page info\u001b[39;49;00m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_valid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_reason\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_reason\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     tables_metadata.append(table_meta)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Create file-level metadata\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mcreate_table_metadata\u001b[39m\u001b[34m(file_id, table_index, csv_path, page_number, table, is_valid, validation_reason, continuation_group, is_continuation, parent_table_id)\u001b[39m\n\u001b[32m     47\u001b[39m table_id = compute_table_id(file_id, page_number, table_index, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, params_hash)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Compute CSV hash\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m csv_hash = compute_file_hash(csv_path) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Basic structure info\u001b[39;00m\n\u001b[32m     53\u001b[39m num_rows = \u001b[38;5;28mlen\u001b[39m(table) \u001b[38;5;28;01mif\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:19\u001b[39m, in \u001b[36mexists\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "# Example usage with metadata\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file = pdf_src\n",
    "    save_dir = \"../data/parsed/tabula_output\"\n",
    "\n",
    "    csv_files, file_metadata, tables_metadata = export_tables_to_csv_with_metadata(pdf_file, save_dir)\n",
    "    \n",
    "    print(\"Exported CSV files:\")\n",
    "    for f in csv_files:\n",
    "        print(f)\n",
    "    \n",
    "    print(f\"\\nFile metadata:\")\n",
    "    print(f\"File ID: {file_metadata['file_id']}\")\n",
    "    print(f\"Tables found: {file_metadata['tables_found']}\")\n",
    "    print(f\"Valid tables: {file_metadata['tables_valid']}\")\n",
    "    \n",
    "    print(f\"\\nTable metadata summary:\")\n",
    "    valid_tables = [t for t in tables_metadata if t['is_valid_table']]\n",
    "    print(f\"Valid tables: {len(valid_tables)}\")\n",
    "    print(f\"Invalid tables: {len(tables_metadata) - len(valid_tables)}\")\n",
    "    \n",
    "    # Show validation reasons\n",
    "    validation_reasons = {}\n",
    "    for t in tables_metadata:\n",
    "        reason = t['validation_reason']\n",
    "        validation_reasons[reason] = validation_reasons.get(reason, 0) + 1\n",
    "    \n",
    "    print(f\"\\nValidation reasons:\")\n",
    "    for reason, count in validation_reasons.items():\n",
    "        print(f\"  {reason}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDF Parser Kernel",
   "language": "python",
   "name": "pdf-parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
