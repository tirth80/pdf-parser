{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDF Parser Evaluation\n",
        "This notebook demonstrates the evaluation system for PDF parsing quality and regression detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Manual Transcription Data and Create Ground Truth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual transcription data shape: (177, 4)\n",
            "\n",
            "Columns: ['PART II', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3']\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PART II</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Item 8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>STOCKHOLDERS’ EQUITY STATEMENTS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(In millions, except per share amounts)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Year Ended June 30,</td>\n",
              "      <td>$2,022.00</td>\n",
              "      <td>$2,021.00</td>\n",
              "      <td>$2,020.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Common stock and paid-in capital</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Balance, beginning of period</td>\n",
              "      <td>$83,111.00</td>\n",
              "      <td>$80,552.00</td>\n",
              "      <td>$78,520.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Common stock issued</td>\n",
              "      <td>$1,841.00</td>\n",
              "      <td>$1,963.00</td>\n",
              "      <td>$1,343.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Common stock repurchased</td>\n",
              "      <td>-$5,688.00</td>\n",
              "      <td>-$5,539.00</td>\n",
              "      <td>-$4,599.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Stock-based compensation expense</td>\n",
              "      <td>$7,502.00</td>\n",
              "      <td>$6,118.00</td>\n",
              "      <td>$5,289.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Other, net</td>\n",
              "      <td>$173.00</td>\n",
              "      <td>$17.00</td>\n",
              "      <td>-$1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   PART II  Unnamed: 1  Unnamed: 2  Unnamed: 3\n",
              "0                                   Item 8         NaN         NaN         NaN\n",
              "1          STOCKHOLDERS’ EQUITY STATEMENTS         NaN         NaN         NaN\n",
              "2  (In millions, except per share amounts)         NaN         NaN         NaN\n",
              "3                      Year Ended June 30,   $2,022.00   $2,021.00   $2,020.00\n",
              "4         Common stock and paid-in capital         NaN         NaN         NaN\n",
              "5             Balance, beginning of period  $83,111.00  $80,552.00  $78,520.00\n",
              "6                      Common stock issued   $1,841.00   $1,963.00   $1,343.00\n",
              "7                 Common stock repurchased  -$5,688.00  -$5,539.00  -$4,599.00\n",
              "8         Stock-based compensation expense   $7,502.00   $6,118.00   $5,289.00\n",
              "9                               Other, net     $173.00      $17.00      -$1.00"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the manual transcription data\n",
        "manual_data_path = '../data/manual_data/MSFT_10-K_20220728_000156459022026876-pages - MSFT_10-K_20220728_000156459022026876-pages.csv.csv'\n",
        "manual_df = pd.read_csv(manual_data_path)\n",
        "\n",
        "print(\"Manual transcription data shape:\", manual_df.shape)\n",
        "print(\"\\nColumns:\", manual_df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "manual_df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created ground truth file for page 61: 29 lines\n",
            "Created ground truth file for page 62: 38 lines\n",
            "Created ground truth file for page 63: 41 lines\n",
            "Created ground truth file for page 64: 30 lines\n",
            "\n",
            "Ground truth files created in: ../data/ground_truth\n"
          ]
        }
      ],
      "source": [
        "# Create ground truth directory and process manual data\n",
        "ground_truth_dir = Path('../data/ground_truth')\n",
        "ground_truth_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Process manual data into ground truth format for pages 61-64\n",
        "page_ranges = {\n",
        "    61: (0, 30),    # Stockholders' equity statements\n",
        "    62: (30, 85),   # Notes to financial statements part 1\n",
        "    63: (85, 140),  # Notes to financial statements part 2\n",
        "    64: (140, -1)   # Notes to financial statements part 3\n",
        "}\n",
        "\n",
        "for page_num, (start_idx, end_idx) in page_ranges.items():\n",
        "    if end_idx == -1:\n",
        "        page_data = manual_df.iloc[start_idx:]\n",
        "    else:\n",
        "        page_data = manual_df.iloc[start_idx:end_idx]\n",
        "    \n",
        "    # Create ground truth CSV for this page\n",
        "    gt_file = ground_truth_dir / f'page_{page_num}_ground_truth.csv'\n",
        "    \n",
        "    # Process the data to create proper ground truth format\n",
        "    ground_truth_rows = []\n",
        "    \n",
        "    for idx, row in page_data.iterrows():\n",
        "        # Extract text from all non-empty columns\n",
        "        text_parts = []\n",
        "        for col in page_data.columns:\n",
        "            val = str(row[col]).strip()\n",
        "            if val and val != 'nan' and val != 'NaN':\n",
        "                text_parts.append(val)\n",
        "        \n",
        "        if text_parts:\n",
        "            ground_truth_rows.append({\n",
        "                'page_number': page_num,\n",
        "                'line_number': len(ground_truth_rows) + 1,\n",
        "                'text': ' '.join(text_parts),\n",
        "                'is_table_cell': True if len(text_parts) > 1 else False,\n",
        "                'original_row': idx\n",
        "            })\n",
        "    \n",
        "    gt_df = pd.DataFrame(ground_truth_rows)\n",
        "    gt_df.to_csv(gt_file, index=False)\n",
        "    print(f\"Created ground truth file for page {page_num}: {len(gt_df)} lines\")\n",
        "\n",
        "print(f\"\\nGround truth files created in: {ground_truth_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Simple Text Extraction Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded parsed content: 365682 characters\n",
            "First 500 characters:\n",
            "# Extracted Text from MSFT_10-K_20220728_000156459022026876.pdf\n",
            "# Processing Date: 2025-09-21T14:41:36.073576\n",
            "# Total Pages: 111\n",
            "# PDFplumber Pages: 109\n",
            "# OCR Pages: 2\n",
            "# Poor Quality Pages: 2\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PAGE 1 | Method: pdfplumber | Quality Score: 100\n",
            "================================================================================\n",
            "\n",
            "UNITED STATES\n",
            "SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "FORM 10-K\n",
            "☒ ANNUAL REP\n",
            "\n",
            "...\n",
            "Last 500 characters:\n",
            " Rodriguez\n",
            "/s/ CHARLES W. SCHARF Director\n",
            "Charles W. Scharf\n",
            "/s/ JOHN W. STANTON Director\n",
            "John W. Stanton\n",
            "/s/ JOHN W. THOMPSON Lead Independent Director\n",
            "John W. Thompson\n",
            "/s/ EMMA N. WALMSLEY Director\n",
            "Emma N. Walmsley\n",
            "/s/ PADMASREE WARRIOR Director\n",
            "Padmasree Warrior\n",
            "/s/ AMY E. HOOD Executive Vice President and Chief Financial Officer\n",
            "Amy E. Hood (Principal Financial Officer)\n",
            "/s/ ALICE L. JOLLA Corporate Vice President and Chief Accounting Officer (Principal\n",
            "Alice L. Jolla Accounting Officer)\n",
            "110\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the parsed text data from simple text extraction\n",
        "parsed_text_file = '../data/parsed/MSFT/2022/MSFT_10-K_20220728_000156459022026876_extracted.txt'\n",
        "with open(parsed_text_file, 'r', encoding='utf-8') as f:\n",
        "    parsed_content = f.read()\n",
        "\n",
        "print(f\"Loaded parsed content: {len(parsed_content)} characters\")\n",
        "print(\"First 500 characters:\")\n",
        "print(parsed_content[:500])\n",
        "print(\"\\n...\")\n",
        "print(\"Last 500 characters:\")\n",
        "print(parsed_content[-500:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted pages: [61, 62, 63, 64]\n",
            "\n",
            "Page 61: 1104 characters\n",
            "First 200 chars: PART II\n",
            "Item 8\n",
            "STOCKHOLDERS’ EQUITY STATEMENTS\n",
            "(In millions, except per share amounts)\n",
            "Year Ended June 30, 2022 2021 2020\n",
            "Common stock and paid-in capital\n",
            "Balance, beginning of period $ 83,111 $ 80,55...\n",
            "\n",
            "Page 62: 3554 characters\n",
            "First 200 chars: PART II\n",
            "Item 8\n",
            "NOTES TO FINANCIAL STATEMENTS\n",
            "NOTE 1 — ACCOUNTING POLICIES\n",
            "Accounting Principles\n",
            "Our consolidated financial statements and accompanying notes are prepared in accordance with accounting ...\n",
            "\n",
            "Page 63: 4485 characters\n",
            "First 200 chars: PART II\n",
            "Item 8\n",
            "Revenue Recognition\n",
            "Revenue is recognized upon transfer of control of promised products or services to customers in an amount that reflects the\n",
            "consideration we expect to receive in exc...\n",
            "\n",
            "Page 64: 4566 characters\n",
            "First 200 chars: PART II\n",
            "Item 8\n",
            "Judgment is required to determine the SSP for each distinct performance obligation. We use a single amount to estimate SSP for\n",
            "items that are not sold separately, including on-premises ...\n"
          ]
        }
      ],
      "source": [
        "# Extract text for pages 61-64 from the parsed content\n",
        "import re\n",
        "\n",
        "# Split content by page markers\n",
        "page_pattern = r'={80,}\\nPAGE (\\d+) \\|.*?\\n={80,}\\n'\n",
        "pages = re.split(page_pattern, parsed_content)\n",
        "\n",
        "# Extract target pages (61-64)\n",
        "target_pages = {}\n",
        "current_page = None\n",
        "\n",
        "for i, section in enumerate(pages):\n",
        "    if section.isdigit():\n",
        "        current_page = int(section)\n",
        "    elif current_page and current_page in [61, 62, 63, 64]:\n",
        "        target_pages[current_page] = section.strip()\n",
        "\n",
        "print(\"Extracted pages:\", list(target_pages.keys()))\n",
        "for page_num in sorted(target_pages.keys()):\n",
        "    print(f\"\\nPage {page_num}: {len(target_pages[page_num])} characters\")\n",
        "    print(f\"First 200 chars: {target_pages[page_num][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Implement WER/CER Calculation and Table Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jiwer already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for WER/CER calculation\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from jiwer import wer, cer\n",
        "    print(\"jiwer already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing jiwer...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"jiwer\"])\n",
        "    from jiwer import wer, cer\n",
        "\n",
        "import difflib\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple, Optional, Any\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class EvaluationMetrics:\n",
        "    \"\"\"Container for evaluation metrics\"\"\"\n",
        "    wer: float\n",
        "    cer: float\n",
        "    table_precision: float\n",
        "    table_recall: float\n",
        "    table_f1: float\n",
        "    text_similarity: float\n",
        "    word_count_accuracy: float\n",
        "    line_count_accuracy: float\n",
        "    extraction_time: float\n",
        "    quality_score: float\n",
        "\n",
        "@dataclass\n",
        "class GroundTruthData:\n",
        "    \"\"\"Container for ground truth data\"\"\"\n",
        "    page_number: int\n",
        "    text: str\n",
        "    tables: List[pd.DataFrame]\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Normalize text for comparison\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to lowercase and remove extra whitespace\n",
        "    normalized = re.sub(r'\\s+', ' ', text.lower().strip())\n",
        "    \n",
        "    # Remove common OCR artifacts and formatting\n",
        "    normalized = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\$\\%\\(\\)]', '', normalized)\n",
        "    \n",
        "    return normalized\n",
        "\n",
        "def evaluate_text_extraction(predicted_text: str, ground_truth_text: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate text extraction quality using various metrics.\n",
        "    \"\"\"\n",
        "    # Normalize texts for comparison\n",
        "    pred_normalized = normalize_text(predicted_text)\n",
        "    gt_normalized = normalize_text(ground_truth_text)\n",
        "    \n",
        "    # Calculate WER and CER\n",
        "    try:\n",
        "        wer_score = wer(gt_normalized, pred_normalized)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: WER calculation failed: {e}\")\n",
        "        wer_score = 1.0\n",
        "    \n",
        "    try:\n",
        "        cer_score = cer(gt_normalized, pred_normalized)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: CER calculation failed: {e}\")\n",
        "        cer_score = 1.0\n",
        "    \n",
        "    # Text similarity using SequenceMatcher\n",
        "    similarity = difflib.SequenceMatcher(None, pred_normalized, gt_normalized).ratio()\n",
        "    \n",
        "    # Word and line count accuracy\n",
        "    pred_words = len(pred_normalized.split())\n",
        "    gt_words = len(gt_normalized.split())\n",
        "    word_count_accuracy = 1 - abs(pred_words - gt_words) / max(gt_words, 1)\n",
        "    \n",
        "    pred_lines = len(predicted_text.split('\\n'))\n",
        "    gt_lines = len(ground_truth_text.split('\\n'))\n",
        "    line_count_accuracy = 1 - abs(pred_lines - gt_lines) / max(gt_lines, 1)\n",
        "    \n",
        "    return {\n",
        "        'wer': wer_score,\n",
        "        'cer': cer_score,\n",
        "        'text_similarity': similarity,\n",
        "        'word_count_accuracy': max(0, word_count_accuracy),\n",
        "        'line_count_accuracy': max(0, line_count_accuracy),\n",
        "        'predicted_word_count': pred_words,\n",
        "        'ground_truth_word_count': gt_words,\n",
        "        'predicted_line_count': pred_lines,\n",
        "        'ground_truth_line_count': gt_lines\n",
        "    }\n",
        "\n",
        "print(\"Evaluation functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Evaluation on Pages 61-64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ground truth for 4 pages\n",
            "Ground truth pages: [61, 62, 63, 64]\n"
          ]
        }
      ],
      "source": [
        "# Load ground truth data for evaluation\n",
        "ground_truth_data = {}\n",
        "\n",
        "for page_num in [61, 62, 63, 64]:\n",
        "    gt_file = ground_truth_dir / f'page_{page_num}_ground_truth.csv'\n",
        "    if gt_file.exists():\n",
        "        df = pd.read_csv(gt_file)\n",
        "        \n",
        "        # Extract text from the ground truth\n",
        "        if 'text' in df.columns:\n",
        "            text = '\\n'.join(df['text'].dropna().astype(str))\n",
        "        else:\n",
        "            # Concatenate all text from all columns\n",
        "            text_parts = []\n",
        "            for col in df.columns:\n",
        "                if df[col].dtype == 'object':  # Text columns\n",
        "                    text_parts.extend(df[col].dropna().astype(str).tolist())\n",
        "            text = ' '.join(text_parts)\n",
        "        \n",
        "        ground_truth_data[page_num] = GroundTruthData(\n",
        "            page_number=page_num,\n",
        "            text=text,\n",
        "            tables=[df] if not df.empty else [],\n",
        "            metadata={'source_file': str(gt_file)}\n",
        "        )\n",
        "\n",
        "print(f\"Loaded ground truth for {len(ground_truth_data)} pages\")\n",
        "print(\"Ground truth pages:\", list(ground_truth_data.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating Page 61 ===\n",
            "WER: 0.453\n",
            "CER: 0.203\n",
            "Text Similarity: 0.580\n",
            "Word Count Accuracy: 0.963\n",
            "Line Count Accuracy: 0.966\n",
            "\n",
            "=== Evaluating Page 62 ===\n",
            "WER: 0.058\n",
            "CER: 0.047\n",
            "Text Similarity: 0.976\n",
            "Word Count Accuracy: 0.957\n",
            "Line Count Accuracy: 0.955\n",
            "\n",
            "=== Evaluating Page 63 ===\n",
            "WER: 0.306\n",
            "CER: 0.279\n",
            "Text Similarity: 0.843\n",
            "Word Count Accuracy: 0.756\n",
            "Line Count Accuracy: 0.865\n",
            "\n",
            "=== Evaluating Page 64 ===\n",
            "WER: 0.534\n",
            "CER: 0.474\n",
            "Text Similarity: 0.808\n",
            "Word Count Accuracy: 0.466\n",
            "Line Count Accuracy: 0.533\n",
            "\n",
            "=== SUMMARY ===\n",
            "Evaluated 4 pages\n"
          ]
        }
      ],
      "source": [
        "# Evaluate each page\n",
        "evaluation_results = []\n",
        "\n",
        "for page_num in [61, 62, 63, 64]:\n",
        "    if page_num in ground_truth_data and page_num in target_pages:\n",
        "        print(f\"\\n=== Evaluating Page {page_num} ===\")\n",
        "        \n",
        "        gt_text = ground_truth_data[page_num].text\n",
        "        pred_text = target_pages[page_num]\n",
        "        \n",
        "        # Evaluate text extraction\n",
        "        text_metrics = evaluate_text_extraction(pred_text, gt_text)\n",
        "        \n",
        "        print(f\"WER: {text_metrics['wer']:.3f}\")\n",
        "        print(f\"CER: {text_metrics['cer']:.3f}\")\n",
        "        print(f\"Text Similarity: {text_metrics['text_similarity']:.3f}\")\n",
        "        print(f\"Word Count Accuracy: {text_metrics['word_count_accuracy']:.3f}\")\n",
        "        print(f\"Line Count Accuracy: {text_metrics['line_count_accuracy']:.3f}\")\n",
        "        \n",
        "        # Store results\n",
        "        page_result = {\n",
        "            'page_number': page_num,\n",
        "            'wer': text_metrics['wer'],\n",
        "            'cer': text_metrics['cer'],\n",
        "            'text_similarity': text_metrics['text_similarity'],\n",
        "            'word_count_accuracy': text_metrics['word_count_accuracy'],\n",
        "            'line_count_accuracy': text_metrics['line_count_accuracy'],\n",
        "            'predicted_word_count': text_metrics['predicted_word_count'],\n",
        "            'ground_truth_word_count': text_metrics['ground_truth_word_count'],\n",
        "            'predicted_line_count': text_metrics['predicted_line_count'],\n",
        "            'ground_truth_line_count': text_metrics['ground_truth_line_count']\n",
        "        }\n",
        "        evaluation_results.append(page_result)\n",
        "\n",
        "print(f\"\\n=== SUMMARY ===\")\n",
        "print(f\"Evaluated {len(evaluation_results)} pages\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Aggregate Metrics and Unit Tests with Thresholds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== AGGREGATE METRICS ===\n",
            "mean_wer: 0.338\n",
            "mean_cer: 0.251\n",
            "mean_text_similarity: 0.802\n",
            "mean_word_count_accuracy: 0.786\n",
            "mean_line_count_accuracy: 0.830\n",
            "std_wer: 0.209\n",
            "std_cer: 0.177\n",
            "min_text_similarity: 0.580\n",
            "max_text_similarity: 0.976\n",
            "pages_evaluated: 4\n",
            "evaluation_timestamp: 2025-09-26T01:34:57.426784\n"
          ]
        }
      ],
      "source": [
        "# Calculate aggregate metrics\n",
        "if evaluation_results:\n",
        "    df_results = pd.DataFrame(evaluation_results)\n",
        "    \n",
        "    aggregate_metrics = {\n",
        "        'mean_wer': df_results['wer'].mean(),\n",
        "        'mean_cer': df_results['cer'].mean(),\n",
        "        'mean_text_similarity': df_results['text_similarity'].mean(),\n",
        "        'mean_word_count_accuracy': df_results['word_count_accuracy'].mean(),\n",
        "        'mean_line_count_accuracy': df_results['line_count_accuracy'].mean(),\n",
        "        'std_wer': df_results['wer'].std(),\n",
        "        'std_cer': df_results['cer'].std(),\n",
        "        'min_text_similarity': df_results['text_similarity'].min(),\n",
        "        'max_text_similarity': df_results['text_similarity'].max(),\n",
        "        'pages_evaluated': len(evaluation_results),\n",
        "        'evaluation_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    print(\"=== AGGREGATE METRICS ===\")\n",
        "    for metric, value in aggregate_metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"{metric}: {value:.3f}\")\n",
        "        else:\n",
        "            print(f\"{metric}: {value}\")\n",
        "else:\n",
        "    print(\"No evaluation results available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n=== QUALITY THRESHOLD TESTS ===\n",
            "Tests Passed: 5\n",
            "Tests Failed: 0\n",
            "Overall Result: PASS\n",
            "\\nDetailed Results:\n",
            "  wer_test: ✅ PASS - WER 0.338 < 0.5\n",
            "  cer_test: ✅ PASS - CER 0.251 < 0.3\n",
            "  similarity_test: ✅ PASS - Text Similarity 0.802 > 0.6\n",
            "  word_accuracy_test: ✅ PASS - Word Accuracy 0.786 > 0.7\n",
            "  line_accuracy_test: ✅ PASS - Line Accuracy 0.830 > 0.5\n"
          ]
        }
      ],
      "source": [
        "# Unit tests with metric thresholds\n",
        "def test_parsing_quality_thresholds(metrics: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Test parsing quality against predefined thresholds.\n",
        "    Returns test results with pass/fail status.\n",
        "    \"\"\"\n",
        "    # Define quality thresholds\n",
        "    thresholds = {\n",
        "        'max_acceptable_wer': 0.5,      # WER should be < 0.5 (50% error rate)\n",
        "        'max_acceptable_cer': 0.3,      # CER should be < 0.3 (30% error rate)\n",
        "        'min_text_similarity': 0.6,     # Text similarity should be > 0.6\n",
        "        'min_word_accuracy': 0.7,       # Word count accuracy should be > 0.7\n",
        "        'min_line_accuracy': 0.5        # Line count accuracy should be > 0.5\n",
        "    }\n",
        "    \n",
        "    test_results = {\n",
        "        'tests_passed': 0,\n",
        "        'tests_failed': 0,\n",
        "        'test_details': {}\n",
        "    }\n",
        "    \n",
        "    # Test WER\n",
        "    wer_pass = metrics['mean_wer'] < thresholds['max_acceptable_wer']\n",
        "    test_results['test_details']['wer_test'] = {\n",
        "        'passed': wer_pass,\n",
        "        'value': metrics['mean_wer'],\n",
        "        'threshold': thresholds['max_acceptable_wer'],\n",
        "        'message': f\"WER {metrics['mean_wer']:.3f} {'<' if wer_pass else '>='} {thresholds['max_acceptable_wer']}\"\n",
        "    }\n",
        "    if wer_pass:\n",
        "        test_results['tests_passed'] += 1\n",
        "    else:\n",
        "        test_results['tests_failed'] += 1\n",
        "    \n",
        "    # Test CER\n",
        "    cer_pass = metrics['mean_cer'] < thresholds['max_acceptable_cer']\n",
        "    test_results['test_details']['cer_test'] = {\n",
        "        'passed': cer_pass,\n",
        "        'value': metrics['mean_cer'],\n",
        "        'threshold': thresholds['max_acceptable_cer'],\n",
        "        'message': f\"CER {metrics['mean_cer']:.3f} {'<' if cer_pass else '>='} {thresholds['max_acceptable_cer']}\"\n",
        "    }\n",
        "    if cer_pass:\n",
        "        test_results['tests_passed'] += 1\n",
        "    else:\n",
        "        test_results['tests_failed'] += 1\n",
        "    \n",
        "    # Test Text Similarity\n",
        "    sim_pass = metrics['mean_text_similarity'] > thresholds['min_text_similarity']\n",
        "    test_results['test_details']['similarity_test'] = {\n",
        "        'passed': sim_pass,\n",
        "        'value': metrics['mean_text_similarity'],\n",
        "        'threshold': thresholds['min_text_similarity'],\n",
        "        'message': f\"Text Similarity {metrics['mean_text_similarity']:.3f} {'>' if sim_pass else '<='} {thresholds['min_text_similarity']}\"\n",
        "    }\n",
        "    if sim_pass:\n",
        "        test_results['tests_passed'] += 1\n",
        "    else:\n",
        "        test_results['tests_failed'] += 1\n",
        "    \n",
        "    # Test Word Count Accuracy\n",
        "    word_pass = metrics['mean_word_count_accuracy'] > thresholds['min_word_accuracy']\n",
        "    test_results['test_details']['word_accuracy_test'] = {\n",
        "        'passed': word_pass,\n",
        "        'value': metrics['mean_word_count_accuracy'],\n",
        "        'threshold': thresholds['min_word_accuracy'],\n",
        "        'message': f\"Word Accuracy {metrics['mean_word_count_accuracy']:.3f} {'>' if word_pass else '<='} {thresholds['min_word_accuracy']}\"\n",
        "    }\n",
        "    if word_pass:\n",
        "        test_results['tests_passed'] += 1\n",
        "    else:\n",
        "        test_results['tests_failed'] += 1\n",
        "    \n",
        "    # Test Line Count Accuracy\n",
        "    line_pass = metrics['mean_line_count_accuracy'] > thresholds['min_line_accuracy']\n",
        "    test_results['test_details']['line_accuracy_test'] = {\n",
        "        'passed': line_pass,\n",
        "        'value': metrics['mean_line_count_accuracy'],\n",
        "        'threshold': thresholds['min_line_accuracy'],\n",
        "        'message': f\"Line Accuracy {metrics['mean_line_count_accuracy']:.3f} {'>' if line_pass else '<='} {thresholds['min_line_accuracy']}\"\n",
        "    }\n",
        "    if line_pass:\n",
        "        test_results['tests_passed'] += 1\n",
        "    else:\n",
        "        test_results['tests_failed'] += 1\n",
        "    \n",
        "    test_results['overall_pass'] = test_results['tests_failed'] == 0\n",
        "    \n",
        "    return test_results\n",
        "\n",
        "# Run the tests\n",
        "if 'aggregate_metrics' in locals():\n",
        "    test_results = test_parsing_quality_thresholds(aggregate_metrics)\n",
        "    \n",
        "    print(\"\\\\n=== QUALITY THRESHOLD TESTS ===\")\n",
        "    print(f\"Tests Passed: {test_results['tests_passed']}\")\n",
        "    print(f\"Tests Failed: {test_results['tests_failed']}\")\n",
        "    print(f\"Overall Result: {'PASS' if test_results['overall_pass'] else 'FAIL'}\")\n",
        "    \n",
        "    print(\"\\\\nDetailed Results:\")\n",
        "    for test_name, details in test_results['test_details'].items():\n",
        "        status = \"✅ PASS\" if details['passed'] else \"❌ FAIL\"\n",
        "        print(f\"  {test_name}: {status} - {details['message']}\")\n",
        "    \n",
        "    if not test_results['overall_pass']:\n",
        "        print(\"\\\\n⚠️  PARSER QUALITY BELOW ACCEPTABLE THRESHOLDS!\")\n",
        "        print(\"Consider improving the text extraction algorithm.\")\n",
        "else:\n",
        "    print(\"No metrics available for testing\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PDF Parser (venv)",
      "language": "python",
      "name": "pdf-parser-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
